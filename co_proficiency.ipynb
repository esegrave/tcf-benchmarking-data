{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import util\n",
    "\n",
    "proficiency_path = './data/co/proficiency/'\n",
    "\n",
    "proficiency_files = [\n",
    "    'co_proficiency_2015_cleaned.csv'\n",
    "    , 'co_proficiency_2016_ela_cleaned.csv'\n",
    "    , 'co_proficiency_2016_math_cleaned.csv'\n",
    "    , 'co_proficiency_2017_cleaned.csv'\n",
    "    , 'co_proficiency_2018_cleaned.csv'\n",
    "    , 'co_proficiency_2019_cleaned.csv'\n",
    "]\n",
    "\n",
    "years = [x[15:19] for x in proficiency_files]\n",
    "\n",
    "df = pd.read_csv(proficiency_path + proficiency_files[0])\n",
    "df['year'] = df.apply(lambda x: years[0], axis=1)\n",
    "\n",
    "for i in range(1, len(proficiency_files)):\n",
    "    df2 = pd.read_csv(proficiency_path + proficiency_files[i])\n",
    "    df2['year'] = df2.apply(lambda x: years[i], axis=1)\n",
    "    df = df.append(df2, ignore_index = True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop All Schools records\n",
    "df = df[~df['school_id'].isna()]\n",
    "df = df[df['school_id'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df = df.rename(columns = {\n",
    "    'Number Approached Expectations': 'Approached Expectations'\n",
    "    , 'Number Did Not Yet Meet Expectations': 'Did Not Yet Meet Expectations'\n",
    "    , 'Number Exceeded Expectations': 'Exceeded Expectations'\n",
    "    , 'Number Met Expectations': 'Met Expectations'\n",
    "    , 'Number Partially Met Expectations': 'Partially Met Expectations'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape scores\n",
    "id_vars = ['year', 'district_id', 'district', 'school_id', 'school', 'grade', 'subject', 'num_tested']\n",
    "value_vars = ['Did Not Yet Meet Expectations', 'Approached Expectations', 'Partially Met Expectations', 'Met Expectations', 'Exceeded Expectations']\n",
    "df = pd.melt(df, id_vars = id_vars, value_vars = value_vars, var_name = 'performance_level', value_name = 'num_at_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean grades\n",
    "df['grade'] = df['grade'].str.replace('ELA Grade ', '')\n",
    "df['grade'] = df['grade'].str.replace('Math Grade ', '')\n",
    "df['grade'] = df['grade'].str.replace('English Language Arts Grade ', '')\n",
    "df['grade'] = df['grade'].str.replace('Mathematics Grade ', '')\n",
    "df['grade'] = df['grade'].str.replace('0', '')\n",
    "df['grade'] = df['grade'].str.replace('*', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean subjects\n",
    "def clean_subjects(row):\n",
    "    if row['subject'] == 'MATH' or row['subject'] == 'Mathematics':\n",
    "        return 'Math'\n",
    "    if row['subject'] == 'English Language Arts':\n",
    "        return 'ELA'\n",
    "    return row['subject']\n",
    "    \n",
    "df['subject'] = df.apply(lambda x: clean_subjects(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add group_state\n",
    "df['group_state'] = df.apply(lambda x: 'All Groups', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subgroup files\n",
    "subgroup_files_2015 = [\n",
    "    'co_proficiency_2015_cleaned_math_migrant.csv'\n",
    "    , 'co_proficiency_2015_cleaned_math_lep.csv'\n",
    "    , 'co_proficiency_2015_cleaned_math_iep.csv'\n",
    "    , 'co_proficiency_2015_cleaned_math_gifted.csv'\n",
    "    , 'co_proficiency_2015_cleaned_math_frl.csv'\n",
    "    , 'co_proficiency_2015_cleaned_math_ethnicity.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_migrant.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_lep.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_iep.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_gifted.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_frl.csv'\n",
    "    , 'co_proficiency_2015_cleaned_ela_ethnicity.csv'\n",
    "]\n",
    "\n",
    "subjects = [x[28:29] for x in subgroup_files]\n",
    "\n",
    "df_sg_2015 = pd.read_csv(proficiency_path + subgroup_files_2015[0])\n",
    "df_sg_2015['subject'] = df_sg_2015.apply(lambda x: subjects[0], axis=1)\n",
    "\n",
    "for i in range(1, len(subgroup_files_2015)):\n",
    "    df2 = pd.read_csv(proficiency_path + subgroup_files_2015[i])\n",
    "    df2['subject'] = df2.apply(lambda x: subjects[i], axis=1)\n",
    "    df_sg_2015 = df_sg_2015.append(df2, ignore_index = True, sort=True)\n",
    "    \n",
    "df_sg_2015['year'] = df_sg_2015.apply(lambda x: '2015', axis=1)\n",
    "\n",
    "subgroup_files = [\n",
    "    'co_proficiency_2016_cleaned_ela_ethnicity.csv'\n",
    "    , 'co_proficiency_2016_cleaned_ela_frl.csv'\n",
    "    , 'co_proficiency_2016_cleaned_ela_gifted.csv'\n",
    "    , 'co_proficiency_2016_cleaned_ela_iep.csv'\n",
    "    , 'co_proficiency_2016_cleaned_ela_lep.csv'\n",
    "    , 'co_proficiency_2016_cleaned_ela_migrant.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_ethnicity.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_frl.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_gifted.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_iep.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_lep.csv'\n",
    "    , 'co_proficiency_2016_cleaned_math_migrant.csv'\n",
    "    , 'co_proficiency_2017_cleaned_ela_ethnicity.csv'\n",
    "    , 'co_proficiency_2017_cleaned_ela_frl.csv'\n",
    "    , 'co_proficiency_2017_cleaned_ela_gifted.csv'\n",
    "    , 'co_proficiency_2017_cleaned_ela_iep.csv'\n",
    "    , 'co_proficiency_2017_cleaned_ela_migrant.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_ethnicity.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_frl.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_gifted.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_iep.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_lep.csv'\n",
    "    , 'co_proficiency_2017_cleaned_math_migrant.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_ethnicity.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_frl.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_gifted.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_iep.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_lep.csv'\n",
    "    , 'co_proficiency_2018_cleaned_ela_migrant.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_ethnicity.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_frl.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_gifted.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_iep.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_lep.csv'\n",
    "    , 'co_proficiency_2018_cleaned_math_migrant.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_ethnicity.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_frl.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_gifted.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_iep.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_lep.csv'\n",
    "    , 'co_proficiency_2019_cleaned_ela_migrant.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_ethnicity.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_frl.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_gifted.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_iep.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_lep.csv'\n",
    "    , 'co_proficiency_2019_cleaned_math_migrant.csv'\n",
    "]\n",
    "\n",
    "years = [x[15:19] for x in subgroup_files]\n",
    "subjects = [x[28:29] for x in subgroup_files]\n",
    "\n",
    "df_sg = pd.read_csv(proficiency_path + subgroup_files[0])\n",
    "df_sg['year'] = df_sg.apply(lambda x: years[0], axis=1)\n",
    "df_sg['subject'] = df_sg.apply(lambda x: subjects[0], axis=1)\n",
    "\n",
    "for i in range(1, len(subgroup_files)):\n",
    "    df2 = pd.read_csv(proficiency_path + subgroup_files[i])\n",
    "    df2['year'] = df2.apply(lambda x: years[i], axis=1)\n",
    "    df2['subject'] = df2.apply(lambda x: subjects[i], axis=1)\n",
    "    df2['filename'] = df2.apply(lambda x: subgroup_files[i], axis = 1)\n",
    "    df_sg = df_sg.append(df2, ignore_index = True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 2015 data\n",
    "df_sg_2015 = df_sg_2015[df_sg_2015['school_id'] != 0]\n",
    "def create_num(row):\n",
    "    try:\n",
    "        return int(round(float(row['% MET OR EXCEEDED EXPECTATIONS']) * float(row['num_tested']) / 100))\n",
    "    except:\n",
    "        return ''\n",
    "df_sg_2015['Met or Exceeded Expectations'] = df_sg_2015.apply(lambda x: create_num(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df_sg = df_sg.rename(columns = {\n",
    "    '# Approached Expectations': 'Approached Expectations'\n",
    "    , '# Did Not Yet Meet Expectations': 'Did Not Yet Meet Expectations'\n",
    "    , '# Exceeded Expectations': 'Exceeded Expectations'\n",
    "    , '# Met Expectations': 'Met Expectations'\n",
    "    , '# Partially Met Expectations': 'Partially Met Expectations'\n",
    "    , '# of Valid Scores': 'num_tested'\n",
    "    , 'District Name': 'district'\n",
    "    , 'District Number': 'district_id'\n",
    "    , 'School Name': 'school'\n",
    "    , 'School Number': 'school_id'\n",
    "    , 'Test': 'grade'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append 2015 to other results\n",
    "df_sg = df_sg.append(df_sg_2015, ignore_index = True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sg.to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records that aren't at the school level\n",
    "df_sg = df_sg[df_sg['school_id'] != 0]\n",
    "df_sg = df_sg[~df_sg['school_id'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean subjects\n",
    "def clean_subjects(row):\n",
    "    if row['subject'] == 'm':\n",
    "        return 'Math'\n",
    "    if row['subject'] == 'e':\n",
    "        return 'ELA'\n",
    "    return row['subject']\n",
    "\n",
    "df_sg['subject'] = df_sg.apply(lambda x: clean_subjects(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean grades\n",
    "df_sg['grade'] = df_sg['grade'].str.replace('ELA Grade ', '')\n",
    "df_sg['grade'] = df_sg['grade'].str.replace('Math Grade ', '')\n",
    "df_sg['grade'] = df_sg['grade'].str.replace('Mathematics Grade ', '')\n",
    "df_sg['grade'] = df_sg['grade'].str.replace('0', '')\n",
    "df_sg['grade'] = df_sg['grade'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape scores\n",
    "id_vars = ['year', 'district_id', 'district', 'school_id', 'school', 'grade', 'group_state', 'subject', 'num_tested']\n",
    "value_vars = ['Did Not Yet Meet Expectations', 'Approached Expectations', 'Partially Met Expectations', 'Met Expectations', 'Exceeded Expectations', 'Met or Exceeded Expectations']\n",
    "df_sg = pd.melt(df_sg, id_vars = id_vars, value_vars = value_vars, var_name = 'performance_level', value_name = 'num_at_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append subgroup scores to main\n",
    "df = df.append(df_sg, ignore_index = True, sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add proficient_tf\n",
    "df['proficient_tf'] = [1 if x == 'Met Expectations' or x == 'Exceeded Expectations' or x == 'Met or Exceeded Expectations'\\\n",
    "                       else 0 for x in df['performance_level']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with no scores\n",
    "df = df[~df['num_tested'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pct_at_level\n",
    "def create_pct(row):\n",
    "    try:\n",
    "        return float(row['num_at_level']) / float(row['num_tested'])\n",
    "    except:\n",
    "        return '*'\n",
    "df['pct_at_level'] = df.apply(lambda x: create_pct(x), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final dataset\n",
    "df.to_csv('./data/finalized/co_proficiency.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
